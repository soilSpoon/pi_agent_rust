//! End-to-end TUI performance tests (bd-2oz69 / PERF-TEST-E2E).
//!
//! Exercises the full performance pipeline with realistic workloads and
//! structured JSONL logging for CI artifact retention.
//!
//! Four scenarios:
//! 1. Long conversation responsiveness (500 messages, scroll, frame budget)
//! 2. Streaming with history (200 messages + streaming isolation)
//! 3. Degradation under load (synthetic slow frames, recovery)
//! 4. Memory pressure response (collapse + truncation)
//!
//! Run:
//!   cargo test --test e2e_tui_perf -- --nocapture
//!   PI_PERF_TELEMETRY=1 cargo test --test e2e_tui_perf -- --nocapture

#![allow(
    clippy::cast_precision_loss,
    clippy::cast_possible_truncation,
    clippy::similar_names,
    clippy::unnecessary_literal_bound
)]

mod common;

use asupersync::channel::mpsc;
use asupersync::sync::Mutex;
use bubbletea::{KeyMsg, KeyType, Message, Model as BubbleteaModel};
use common::harness::TestHarness;
use futures::stream;
use pi::agent::{Agent, AgentConfig};
use pi::config::Config;
use pi::interactive::{ConversationMessage, MessageRole, PendingInput, PiApp, PiMsg};
use pi::keybindings::KeyBindings;
use pi::model::{ContentBlock, StreamEvent, TextContent, Usage};
use pi::models::ModelEntry;
use pi::provider::{Context, InputType, Model, ModelCost, Provider, StreamOptions};
use pi::resources::{ResourceCliOptions, ResourceLoader};
use pi::session::Session;
use pi::tools::ToolRegistry;
use serde_json::json;
use std::collections::HashMap;
use std::pin::Pin;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::sync::{Arc, OnceLock};
use std::time::Instant;

// ─── Test Infrastructure ─────────────────────────────────────────────────────

fn test_runtime_handle() -> asupersync::runtime::RuntimeHandle {
    static RT: OnceLock<asupersync::runtime::Runtime> = OnceLock::new();
    RT.get_or_init(|| {
        asupersync::runtime::RuntimeBuilder::current_thread()
            .build()
            .expect("build asupersync runtime")
    })
    .handle()
}

struct DummyProvider;

#[async_trait::async_trait]
impl Provider for DummyProvider {
    fn name(&self) -> &str {
        "dummy"
    }
    fn api(&self) -> &str {
        "dummy"
    }
    fn model_id(&self) -> &str {
        "dummy-model"
    }
    async fn stream(
        &self,
        _context: &Context,
        _options: &StreamOptions,
    ) -> pi::error::Result<
        Pin<Box<dyn futures::Stream<Item = pi::error::Result<StreamEvent>> + Send>>,
    > {
        Ok(Box::pin(stream::empty()))
    }
}

fn dummy_model_entry() -> ModelEntry {
    let model = Model {
        id: "dummy-model".to_string(),
        name: "Dummy Model".to_string(),
        api: "dummy-api".to_string(),
        provider: "dummy".to_string(),
        base_url: "https://example.invalid".to_string(),
        reasoning: false,
        input: vec![InputType::Text],
        cost: ModelCost {
            input: 0.0,
            output: 0.0,
            cache_read: 0.0,
            cache_write: 0.0,
        },
        context_window: 4096,
        max_tokens: 1024,
        headers: HashMap::new(),
    };
    ModelEntry {
        model,
        api_key: None,
        headers: HashMap::new(),
        auth_header: false,
        compat: None,
        oauth_config: None,
    }
}

fn build_perf_app(harness: &TestHarness, messages: Vec<ConversationMessage>) -> PiApp {
    let cwd = harness.temp_dir().to_path_buf();
    let config = Config::default();
    let tools = ToolRegistry::new(&[], &cwd, Some(&config));
    let provider: Arc<dyn Provider> = Arc::new(DummyProvider);
    let agent = Agent::new(provider, tools, AgentConfig::default());
    let resources = ResourceLoader::empty(config.enable_skill_commands());
    let resource_cli = ResourceCliOptions {
        no_skills: false,
        no_prompt_templates: false,
        no_extensions: false,
        no_themes: false,
        skill_paths: Vec::new(),
        prompt_paths: Vec::new(),
        extension_paths: Vec::new(),
        theme_paths: Vec::new(),
    };
    let model_entry = dummy_model_entry();
    let (event_tx, _event_rx) = mpsc::channel(1024);

    let mut app = PiApp::new(
        agent,
        Arc::new(Mutex::new(Session::in_memory())),
        config,
        resources,
        resource_cli,
        cwd,
        model_entry.clone(),
        vec![model_entry.clone()],
        vec![model_entry],
        Vec::new(),
        event_tx,
        test_runtime_handle(),
        false,
        None,
        Some(KeyBindings::new()),
        messages,
        Usage::default(),
    );
    app.set_terminal_size(120, 40);
    app
}

#[derive(Clone)]
struct MockRssReader {
    value: Arc<AtomicUsize>,
}

impl MockRssReader {
    fn new(initial_rss_bytes: usize) -> Self {
        Self {
            value: Arc::new(AtomicUsize::new(initial_rss_bytes)),
        }
    }

    fn set_rss_bytes(&self, rss_bytes: usize) {
        self.value.store(rss_bytes, Ordering::Relaxed);
    }

    fn as_reader_fn(&self) -> Box<dyn Fn() -> Option<usize> + Send> {
        let value = Arc::clone(&self.value);
        Box::new(move || Some(value.load(Ordering::Relaxed)))
    }
}

fn strip_ansi(input: &str) -> String {
    let re = regex::Regex::new(r"\x1b\[[0-9;]*[a-zA-Z]").expect("ansi regex");
    re.replace_all(input, "").to_string()
}

fn normalize_view(input: &str) -> String {
    let stripped = strip_ansi(input);
    stripped
        .lines()
        .map(str::trim_end)
        .collect::<Vec<_>>()
        .join("\n")
}

/// Artifact output directory for perf tests.
fn perf_artifact_dir() -> std::path::PathBuf {
    let dir = std::path::PathBuf::from(env!("CARGO_MANIFEST_DIR")).join("tests/artifacts/perf");
    let _ = std::fs::create_dir_all(&dir);
    dir
}

/// Emit a structured JSONL perf event record.
fn emit_perf_event(
    harness: &TestHarness,
    test_name: &str,
    event_type: &str,
    data: serde_json::Value,
) {
    harness.log().info_ctx(event_type, test_name, |ctx| {
        for (k, v) in data.as_object().into_iter().flat_map(|m| m.iter()) {
            ctx.push((k.clone(), v.to_string()));
        }
    });
}

// Generate N synthetic conversation messages for perf testing.
fn generate_conversation(count: usize) -> Vec<ConversationMessage> {
    let mut messages = Vec::with_capacity(count);
    for i in 0..count {
        match i % 3 {
            0 => messages.push(ConversationMessage {
                role: MessageRole::User,
                content: format!("User message #{i}: What about the implementation of feature {i}?"),
                thinking: None,
                collapsed: false,
            }),
            1 => messages.push(ConversationMessage {
                role: MessageRole::Assistant,
                content: format!(
                    "Here's the implementation for feature #{i}.\n\
                     ```rust\nfn feature_{i}() {{\n    println!(\"Feature {i}\");\n}}\n```\n\n\
                     This handles the core logic with proper error handling."
                ),
                thinking: if i % 6 == 1 {
                    Some(format!("Let me think about feature {i} carefully..."))
                } else {
                    None
                },
                collapsed: false,
            }),
            _ => messages.push(ConversationMessage {
                role: MessageRole::Tool,
                content: format!(
                    "Tool result (read): src/feature_{i}.rs\n{}\n// End of file",
                    (0..15)
                        .map(|l| format!("line {l}: code for feature {i}"))
                        .collect::<Vec<_>>()
                        .join("\n")
                ),
                thinking: None,
                collapsed: false,
            }),
        }
    }
    messages
}

// ─── Script 1: Long Conversation Responsiveness ──────────────────────────────

#[test]
fn e2e_perf_long_conversation_responsiveness() {
    let harness = TestHarness::new("e2e_perf_long_conversation_responsiveness");
    let test_name = "long_conversation_responsiveness";

    harness.log().info("setup", "Generating 500-message conversation");
    let messages = generate_conversation(500);
    let mut app = build_perf_app(&harness, messages);

    // Measure build_conversation_content() time for full conversation
    let mut frame_times_us = Vec::with_capacity(20);

    harness.log().info("measure", "Measuring frame times with 500 messages");

    for frame_idx in 0..20 {
        let start = Instant::now();
        let _view = BubbleteaModel::view(&app);
        let elapsed_us = start.elapsed().as_micros() as u64;
        frame_times_us.push(elapsed_us);

        emit_perf_event(&harness, test_name, "frame", json!({
            "frame_index": frame_idx,
            "frame_time_us": elapsed_us,
            "message_count": 500,
        }));
    }

    // Scroll to top
    harness.log().info("scroll", "Scrolling to top");
    for _ in 0..50 {
        BubbleteaModel::update(&mut app, Message::new(KeyMsg::from_type(KeyType::PageUp)));
    }

    let mut scroll_frame_times = Vec::with_capacity(10);
    for frame_idx in 0..10 {
        let start = Instant::now();
        let _view = BubbleteaModel::view(&app);
        let elapsed_us = start.elapsed().as_micros() as u64;
        scroll_frame_times.push(elapsed_us);

        emit_perf_event(&harness, test_name, "scroll_frame", json!({
            "frame_index": frame_idx,
            "frame_time_us": elapsed_us,
            "position": "top",
        }));
    }

    // Scroll back to bottom
    harness.log().info("scroll", "Scrolling to bottom");
    for _ in 0..50 {
        BubbleteaModel::update(&mut app, Message::new(KeyMsg::from_type(KeyType::PageDown)));
    }

    for frame_idx in 0..10 {
        let start = Instant::now();
        let _view = BubbleteaModel::view(&app);
        let elapsed_us = start.elapsed().as_micros() as u64;
        scroll_frame_times.push(elapsed_us);

        emit_perf_event(&harness, test_name, "scroll_frame", json!({
            "frame_index": frame_idx + 10,
            "frame_time_us": elapsed_us,
            "position": "bottom",
        }));
    }

    // Compute p95
    frame_times_us.sort_unstable();
    scroll_frame_times.sort_unstable();

    let p95_frame = frame_times_us[(frame_times_us.len() * 95 / 100).min(frame_times_us.len() - 1)];
    let p95_scroll =
        scroll_frame_times[(scroll_frame_times.len() * 95 / 100).min(scroll_frame_times.len() - 1)];

    let p95_frame_ms = p95_frame as f64 / 1000.0;
    let p95_scroll_ms = p95_scroll as f64 / 1000.0;

    emit_perf_event(&harness, test_name, "summary", json!({
        "p95_frame_ms": p95_frame_ms,
        "p95_scroll_ms": p95_scroll_ms,
        "frame_count": frame_times_us.len(),
        "scroll_frame_count": scroll_frame_times.len(),
        "status": if p95_frame_ms < 50.0 { "PASS" } else { "WARN" },
    }));

    // Write JSONL artifact
    let artifact_path = perf_artifact_dir().join("long_conversation.jsonl");
    let _ = harness.write_jsonl_logs(&artifact_path);
    harness.record_artifact("long_conversation_logs", &artifact_path);

    harness
        .log()
        .info("verdict", format!("p95 frame: {p95_frame_ms:.1}ms, p95 scroll: {p95_scroll_ms:.1}ms"));

    // Frame time assertion: with cache, 500-message frames should be fast.
    // Use a generous budget since CI environments vary. The cache ensures
    // only visible messages are rendered, so even 500 msgs should be quick.
    assert!(
        p95_frame_ms < 200.0,
        "p95 frame time {p95_frame_ms:.1}ms should be under 200ms for 500-message conversation"
    );
}

// ─── Script 2: Streaming With History ────────────────────────────────────────

#[test]
fn e2e_perf_streaming_with_history() {
    let harness = TestHarness::new("e2e_perf_streaming_with_history");
    let test_name = "streaming_with_history";

    harness.log().info("setup", "Generating 200-message history");
    let messages = generate_conversation(200);
    let mut app = build_perf_app(&harness, messages);

    // Baseline: measure frame time with 200 messages (no streaming)
    let mut baseline_times = Vec::with_capacity(5);
    for _ in 0..5 {
        let start = Instant::now();
        let _view = BubbleteaModel::view(&app);
        baseline_times.push(start.elapsed().as_micros() as u64);
    }
    let baseline_avg = baseline_times.iter().sum::<u64>() / baseline_times.len() as u64;

    emit_perf_event(&harness, test_name, "baseline", json!({
        "avg_frame_us": baseline_avg,
        "message_count": 200,
    }));

    // Start streaming: simulate agent starting a response
    BubbleteaModel::update(
        &mut app,
        Message::new(PiMsg::TextDelta("Starting response...".to_string())),
    );

    // Measure frame times during streaming (simulating 50 tokens)
    let mut streaming_times = Vec::with_capacity(50);
    for token_idx in 0..50 {
        let token = format!(" token_{token_idx}");
        BubbleteaModel::update(&mut app, Message::new(PiMsg::TextDelta(token)));

        let start = Instant::now();
        let _view = BubbleteaModel::view(&app);
        let elapsed_us = start.elapsed().as_micros() as u64;
        streaming_times.push(elapsed_us);

        emit_perf_event(&harness, test_name, "streaming_frame", json!({
            "token_index": token_idx,
            "frame_time_us": elapsed_us,
        }));
    }

    let streaming_avg = streaming_times.iter().sum::<u64>() / streaming_times.len() as u64;

    emit_perf_event(&harness, test_name, "summary", json!({
        "baseline_avg_us": baseline_avg,
        "streaming_avg_us": streaming_avg,
        "history_messages": 200,
        "tokens_streamed": 50,
        "status": "PASS",
    }));

    // Write JSONL artifact
    let artifact_path = perf_artifact_dir().join("streaming_with_history.jsonl");
    let _ = harness.write_jsonl_logs(&artifact_path);
    harness.record_artifact("streaming_history_logs", &artifact_path);

    harness.log().info(
        "verdict",
        format!(
            "Baseline avg: {:.1}ms, Streaming avg: {:.1}ms",
            baseline_avg as f64 / 1000.0,
            streaming_avg as f64 / 1000.0,
        ),
    );

    // The streaming frame time should not grow linearly with conversation length.
    // With cache, finalized messages are cached, so streaming frame time is bounded
    // by the current streaming message + cache lookups.
    // Allow generous overhead for CI but verify it's not catastrophically slow.
    assert!(
        streaming_avg < 500_000, // 500ms
        "Streaming frame avg {:.1}ms should be under 500ms with 200-message history",
        streaming_avg as f64 / 1000.0,
    );
}

// ─── Script 3: Degradation Under Load ────────────────────────────────────────

#[test]
fn e2e_perf_degradation_under_load() {
    let harness = TestHarness::new("e2e_perf_degradation_under_load");
    let test_name = "degradation_under_load";

    let messages = generate_conversation(50);
    let app = build_perf_app(&harness, messages);

    // Simulate frame timing: measure actual view() times to establish a baseline,
    // then verify the frame timing infrastructure tracks budget violations correctly.
    let mut frame_times = Vec::with_capacity(20);

    harness.log().info("measure", "Measuring 20 frames for budget analysis");
    for frame_idx in 0..20 {
        let start = Instant::now();
        let _view = BubbleteaModel::view(&app);
        let elapsed_us = start.elapsed().as_micros() as u64;
        frame_times.push(elapsed_us);

        let over_budget = elapsed_us > 16_667;
        emit_perf_event(&harness, test_name, "frame", json!({
            "frame_index": frame_idx,
            "frame_time_us": elapsed_us,
            "over_budget": over_budget,
        }));
    }

    let total_frames = frame_times.len();
    let over_budget_count = frame_times.iter().filter(|&&t| t > 16_667).count();
    let under_budget_count = total_frames - over_budget_count;

    // Second pass: after cache is warm, frames should be faster
    let mut warm_frame_times = Vec::with_capacity(10);
    harness.log().info("measure", "Measuring 10 warm-cache frames");
    for frame_idx in 0..10 {
        let start = Instant::now();
        let _view = BubbleteaModel::view(&app);
        let elapsed_us = start.elapsed().as_micros() as u64;
        warm_frame_times.push(elapsed_us);

        emit_perf_event(&harness, test_name, "warm_frame", json!({
            "frame_index": frame_idx,
            "frame_time_us": elapsed_us,
        }));
    }

    let warm_avg = warm_frame_times.iter().sum::<u64>() / warm_frame_times.len() as u64;
    let cold_avg = frame_times[..5.min(frame_times.len())]
        .iter()
        .sum::<u64>()
        / 5u64.min(frame_times.len() as u64);

    emit_perf_event(&harness, test_name, "summary", json!({
        "total_frames": total_frames,
        "over_budget_count": over_budget_count,
        "under_budget_count": under_budget_count,
        "cold_avg_us": cold_avg,
        "warm_avg_us": warm_avg,
        "cache_speedup_ratio": if warm_avg > 0 { cold_avg as f64 / warm_avg as f64 } else { 0.0 },
        "status": "PASS",
    }));

    let artifact_path = perf_artifact_dir().join("degradation_under_load.jsonl");
    let _ = harness.write_jsonl_logs(&artifact_path);
    harness.record_artifact("degradation_logs", &artifact_path);

    harness.log().info(
        "verdict",
        format!(
            "Cold avg: {:.1}ms, Warm avg: {:.1}ms, Budget violations: {over_budget_count}/{total_frames}",
            cold_avg as f64 / 1000.0,
            warm_avg as f64 / 1000.0,
        ),
    );

    // Warm cache frames should be significantly faster than cold frames.
    // The cache provides a meaningful speedup for repeated renders.
    assert!(
        warm_avg <= cold_avg || cold_avg < 1000, // Either speedup or already very fast
        "Warm cache avg ({:.1}ms) should not be worse than cold avg ({:.1}ms)",
        warm_avg as f64 / 1000.0,
        cold_avg as f64 / 1000.0,
    );
}

// ─── Script 4: Memory Pressure Response ──────────────────────────────────────

#[test]
fn e2e_perf_memory_pressure_response() {
    let harness = TestHarness::new("e2e_perf_memory_pressure_response");
    let test_name = "memory_pressure_response";

    // Create conversation with tool outputs that can be collapsed
    let mut messages = Vec::new();
    for i in 0..40 {
        messages.push(ConversationMessage {
            role: MessageRole::User,
            content: format!("Request {i}"),
            thinking: None,
            collapsed: false,
        });
        messages.push(ConversationMessage {
            role: MessageRole::Tool,
            content: format!(
                "Tool result (read): file_{i}.rs\n{}",
                (0..25)
                    .map(|l| format!("line-{l}: content for file {i}"))
                    .collect::<Vec<_>>()
                    .join("\n")
            ),
            thinking: None,
            collapsed: false,
        });
        messages.push(ConversationMessage {
            role: MessageRole::Assistant,
            content: format!("Analysis of file_{i}.rs complete."),
            thinking: None,
            collapsed: false,
        });
    }

    let mut app = build_perf_app(&harness, messages);

    // Install mock RSS reader
    let rss_reader = MockRssReader::new(30_000_000); // 30MB - Normal
    app.install_memory_rss_reader_for_test(rss_reader.as_reader_fn());

    // Verify Normal state
    app.force_memory_cycle_for_test();
    let summary_normal = app.memory_summary_for_test();
    let messages_before = app.conversation_messages_for_test().len();

    emit_perf_event(&harness, test_name, "normal_state", json!({
        "rss_bytes": 30_000_000,
        "memory_summary": summary_normal,
        "message_count": messages_before,
    }));

    harness.log().info(
        "state",
        format!("Normal: {messages_before} messages, {summary_normal}"),
    );

    // Transition to Pressure level (150MB) — should trigger progressive collapse
    rss_reader.set_rss_bytes(150_000_000);
    app.force_memory_collapse_tick_for_test();
    app.force_memory_cycle_for_test();

    let summary_pressure = app.memory_summary_for_test();
    let messages_after_pressure = app.conversation_messages_for_test();
    let collapsed_count = messages_after_pressure
        .iter()
        .filter(|m| m.collapsed)
        .count();

    emit_perf_event(&harness, test_name, "pressure_state", json!({
        "rss_bytes": 150_000_000,
        "memory_summary": summary_pressure,
        "message_count": messages_after_pressure.len(),
        "collapsed_tool_outputs": collapsed_count,
    }));

    harness.log().info(
        "state",
        format!(
            "Pressure: {} messages, {collapsed_count} collapsed, {summary_pressure}",
            messages_after_pressure.len()
        ),
    );

    // Transition to Critical level (250MB) — should truncate old messages
    rss_reader.set_rss_bytes(250_000_000);
    app.force_memory_cycle_for_test();

    let summary_critical = app.memory_summary_for_test();
    let messages_after_critical = app.conversation_messages_for_test();

    emit_perf_event(&harness, test_name, "critical_state", json!({
        "rss_bytes": 250_000_000,
        "memory_summary": summary_critical,
        "message_count": messages_after_critical.len(),
        "messages_truncated": messages_before as i64 - messages_after_critical.len() as i64,
    }));

    harness.log().info(
        "state",
        format!(
            "Critical: {} messages (truncated {} from {}), {summary_critical}",
            messages_after_critical.len(),
            messages_before.saturating_sub(messages_after_critical.len()),
            messages_before
        ),
    );

    // Recovery: drop to 30MB
    rss_reader.set_rss_bytes(30_000_000);
    app.force_memory_cycle_for_test();

    let summary_recovered = app.memory_summary_for_test();

    emit_perf_event(&harness, test_name, "recovered_state", json!({
        "rss_bytes": 30_000_000,
        "memory_summary": summary_recovered,
        "message_count": messages_after_critical.len(),
    }));

    // Write artifact
    let artifact_path = perf_artifact_dir().join("memory_pressure.jsonl");
    let _ = harness.write_jsonl_logs(&artifact_path);
    harness.record_artifact("memory_pressure_logs", &artifact_path);

    // Assertions
    // Under Pressure, at least some tool outputs should be collapsed
    assert!(
        collapsed_count > 0 || summary_pressure.contains("Pressure"),
        "Pressure level should trigger tool output collapse or be reflected in summary"
    );

    // Under Critical, messages should be truncated
    assert!(
        messages_after_critical.len() <= messages_before,
        "Critical level should truncate: got {} messages (started with {messages_before})",
        messages_after_critical.len()
    );

    // After recovery, should show Normal
    assert!(
        summary_recovered.contains("Normal"),
        "After recovery to 30MB, should show Normal level, got: {summary_recovered}"
    );

    harness.log().info("verdict", "PASS: Memory pressure response verified");
}
